"""
ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ollama
"""
import os
import asyncio
import aiohttp
import logging
import hashlib
from typing import Dict, Any, Optional
from datetime import datetime

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ°
logger = logging.getLogger(__name__)

class OllamaHandler:
    def __init__(self):
        self.model_name = os.getenv('OLLAMA_MODEL', 'hackathon-assistant:latest')
        self.host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        self.timeout = int(os.getenv('RESPONSE_TIMEOUT', 350))
        
        self._response_cache = {} # ĞšÑÑˆ Ğ´Ğ»Ñ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
        self._model_loaded = False # Ğ¤Ğ»Ğ°Ğ³ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        
        logger.info(f"OllamaHandler Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {self.model_name}")
    
    async def initialize(self):
        try:
            logger.info(f"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {self.model_name}...")
            if not await self.test_connection():
                logger.error("âŒ Ollama Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½! ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ, Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ğ»Ğ¸ ollama serve")
                return False
            model_exists = await self._check_model_exists()
            if model_exists:
                logger.info(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ {self.model_name} Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°")
                self._model_loaded = True
                return True
            else:
                logger.error(f"âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ {self.model_name} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ² Ollama!")
                return False
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: {e}")
            return False
    
    async def _check_model_exists(self) -> bool:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.host}/api/tags", 
                    timeout=5
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('models', [])
                        
                        for model in models:
                            if model.get('name') == self.model_name:
                                return True
                        return False
                    return False
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: {e}")
            return False
    
    async def ask(self, question: str, user_context: Optional[Dict] = None) -> Dict[str, Any]:
        start_time = datetime.now()
        cache_key = self._get_cache_key(question) # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºÑÑˆ Ğ´Ğ»Ñ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
        if cache_key in self._response_cache:
            cached = self._response_cache[cache_key]
            logger.info(f"ğŸ”„ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚")
            return cached
        try:
            logger.info(f"ğŸ“¤ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: '{question[:50]}...'")
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.host}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": question,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 200,
                            "num_thread": 4,
                            "top_k": 40,
                            "top_p": 0.9,
                            "repeat_penalty": 1.1,
                        }
                    },
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                ) as response:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    if response.status == 200:
                        data = await response.json()
                        answer = data.get('response', '').strip()
                        logger.info(f"âœ… ĞÑ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ Ğ·Ğ° {elapsed:.2f}Ñ")
                        result = {
                            'success': True,
                            'answer': answer,
                            'model': self.model_name,
                            'response_time': f"{elapsed:.2f}Ñ",
                            'timestamp': datetime.now().isoformat(),
                        }
                        
                        # ĞšÑÑˆĞ¸Ñ€ÑƒĞµĞ¼ Ñ‡Ğ°ÑÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹
                        if self._should_cache(question):
                            self._response_cache[cache_key] = result
                        return result
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° API: {response.status}")
                        return {
                            'success': False,
                            'answer': f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°",
                            'error': 'api_error',
                            'response_time': f"{elapsed:.2f}Ñ"
                        }
                        
        except asyncio.TimeoutError:
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.error(f"Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ñ‡ĞµÑ€ĞµĞ· {elapsed:.2f}Ñ")
            return {
                'success': False,
                'answer': "â° Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ½ÑĞ»Ğ° ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.",
                'error': 'timeout',
                'response_time': f"{elapsed:.2f}Ñ"
            }
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            return {
                'success': False,
                'answer': "âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº AI.",
                'error': str(e),
                'response_time': f"{elapsed:.2f}Ñ"
            }
    
    def _get_cache_key(self, question: str) -> str:
        normalized = question.lower().strip()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]
    
    def _should_cache(self, question: str) -> bool:
        question_lower = question.lower()
        cache_keywords = [
            'ĞºĞ¾Ğ³Ğ´Ğ°', 'Ğ³Ğ´Ğµ', 'ÑĞºĞ¾Ğ»ÑŒĞºĞ¾', 'ĞºĞ°Ğº', 'Ñ‚ĞµĞ¼Ñ‹', 
            'Ğ¿Ñ€Ğ¸Ğ·Ñ‹', 'ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹', 'Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾', 'Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ',
            'Ñ…Ğ°ĞºĞ°Ñ‚Ğ¾Ğ½', 'Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ', 'Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸', 'Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ',
            'Ğ´Ğ»Ğ¸Ñ‚ÑÑ', 'Ğ²Ñ€ĞµĞ¼Ñ', 'ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ', 'Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ',
            'ÑÑ‚Ğ¾Ğ¸Ñ‚', 'Ñ†ĞµĞ½Ğ°', 'Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾', 'Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚ÑŒ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ',
            'Ğ¼ĞµÑÑ‚Ğ¾', 'Ğ°Ğ´Ñ€ĞµÑ', 'Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚', 'Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€'
        ]
        return any(keyword in question_lower for keyword in cache_keywords)
    
    async def test_connection(self) -> bool:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.host}/", 
                    timeout=3
                ) as response:
                    return response.status == 200
        except Exception as e:
            logger.debug(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğº Ollama")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            'name': self.model_name,
            'loaded': self._model_loaded,
            'cache_size': len(self._response_cache),
        }
    
    def clear_cache(self):
        self._response_cache.clear()
        logger.info("ğŸ—‘ï¸ ĞšÑÑˆ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½")


_assistant_instance = None

def get_assistant() -> OllamaHandler:
    global _assistant_instance
    if _assistant_instance is None:
        _assistant_instance = OllamaHandler()
    return _assistant_instance