"""
–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama
"""
import os
import asyncio
import aiohttp
import json
import logging
import hashlib
from typing import Dict, Any, Optional
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OllamaHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Ollama"""
    
    def __init__(self):
        self.model_name = os.getenv('OLLAMA_MODEL', 'hackathon-assistant:latest')
        self.host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        self.timeout = int(os.getenv('RESPONSE_TIMEOUT', 30))
        
        # –ö—ç—à –¥–ª—è —á–∞—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        self._response_cache = {}
        
        # –§–ª–∞–≥ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        self._model_loaded = False
        
        # –§–ª–∞–≥ –ø—Ä–æ–≥—Ä–µ–≤–∞ –º–æ–¥–µ–ª–∏
        self._warmup_task = None
        
        logger.info(f"OllamaHandler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.model_name}")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è - –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è event loop"""
        await self._warmup_model()
    
    async def _warmup_model(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ - –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–µ –≤ –ø–∞–º—è—Ç—å Ollama –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
        try:
            logger.info(f"üî• –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ {self.model_name}...")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            if not await self.test_connection():
                logger.warning("‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≥—Ä–µ–≤")
                return
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
            warmup_prompt = "hello"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.host}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": warmup_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 5,  # –í—Å–µ–≥–æ 1 —Ç–æ–∫–µ–Ω –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
                            "num_thread": 4,
                        }
                    },
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        self._model_loaded = True
                        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} –ø—Ä–æ–≥—Ä–µ—Ç–∞ –∏ –≥–æ—Ç–æ–≤–∞")
                    else:
                        error_text = await response.text()
                        logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ–≥—Ä–µ—Ç–∞, —Å—Ç–∞—Ç—É—Å: {response.status}, –æ—à–∏–±–∫–∞: {error_text[:100]}")
        except asyncio.TimeoutError:
            logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—Ä–æ–≥—Ä–µ–≤–µ –º–æ–¥–µ–ª–∏")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞ –º–æ–¥–µ–ª–∏: {e}")
    
    async def ask(self, question: str, user_context: Optional[Dict] = None) -> Dict[str, Any]:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –º–æ–¥–µ–ª–∏"""
        start_time = datetime.now()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è —á–∞—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        cache_key = self._get_cache_key(question)
        if cache_key in self._response_cache:
            cached = self._response_cache[cache_key]
            logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
            return cached
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_prompt(question, user_context)
            
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {self.model_name}: '{question[:50]}...'")
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è CPU –≤ Codespace
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.host}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 100,
                            "num_thread": 4,
                            "top_k": 20,
                            "top_p": 0.9,
                            "repeat_penalty": 1.1,
                            "seed": 42
                        }
                    },
                    timeout=aiohttp.ClientTimeout(total=350)
                ) as response:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    
                    if response.status == 200:
                        data = await response.json()
                        answer = data.get('response', '').strip()
                        
                        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.2f}—Å")
                        
                        result = {
                            'success': True,
                            'answer': answer,
                            'model': self.model_name,
                            'response_time': f"{elapsed:.2f}—Å",
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # –ö—ç—à–∏—Ä—É–µ–º —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
                        if self._should_cache(question):
                            self._response_cache[cache_key] = result
                            logger.info(f"üíæ –û—Ç–≤–µ—Ç –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω")
                        
                        return result
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ API: {response.status} - {error_text[:100]}")
                        raise Exception(f"API error {response.status}")
                        
        except asyncio.TimeoutError:
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç —á–µ—Ä–µ–∑ {elapsed:.2f}—Å")
            return {
                'success': False,
                'answer': "‚è∞ –ò–∑–≤–∏–Ω–∏—Ç–µ, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.",
                'error': 'timeout',
                'response_time': f"{elapsed:.2f}—Å"
            }
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return {
                'success': False,
                'answer': f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω.",
                'error': str(e),
                'response_time': f"{elapsed:.2f}—Å"
            }
    
    def _get_cache_key(self, question: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        normalized = question.lower().strip()
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def _should_cache(self, question: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å—Ç–æ–∏—Ç –ª–∏ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å"""
        question_lower = question.lower()
        
        cache_keywords = [
            '–∫–æ–≥–¥–∞', '–≥–¥–µ', '—Å–∫–æ–ª—å–∫–æ', '–∫–∞–∫', '—Ç–µ–º—ã', 
            '–ø—Ä–∏–∑—ã', '–∫–æ–º–∞–Ω–¥—ã', '–Ω–∞—á–∞–ª–æ', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ',
            '—Ö–∞–∫–∞—Ç–æ–Ω', '—á—Ç–æ —Ç–∞–∫–æ–µ', '–º–æ–∂–Ω–æ –ª–∏', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'
        ]
        
        return any(keyword in question_lower for keyword in cache_keywords)
    
    def _build_prompt(self, question: str, user_context: Optional[Dict] = None) -> str:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞"""
        prompt = f"""–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ö–∞–∫–∞—Ç–æ–Ω–∞. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ, 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
        
        return prompt
    
    async def test_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.host}/api/tags", 
                    timeout=5
                ) as response:
                    if response.status == 200:
                        return True
                    return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        return {
            'name': self.model_name,
            'loaded': self._model_loaded,
            'cache_size': len(self._response_cache)
        }
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        self._response_cache.clear()
        logger.info("üóëÔ∏è –ö—ç—à –æ—á–∏—â–µ–Ω")


# –°–∏–Ω–≥–ª—Ç–æ–Ω
_assistant_instance = None

def get_assistant() -> OllamaHandler:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
    global _assistant_instance
    if _assistant_instance is None:
        _assistant_instance = OllamaHandler()
    return _assistant_instance